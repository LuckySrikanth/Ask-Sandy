"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.chat = exports.completion = void 0;
const llm_api_1 = require("llm-api");
const lodash_1 = require("lodash");
const zod_1 = require("zod");
const utils_1 = require("./utils");
const FunctionName = 'print';
const FunctionDescription = 'Respond by calling this function with the correct parameters.';
const Defaults = {
    autoHeal: true,
    autoSlice: false,
};
async function completion(model, prompt, opt) {
    const message = typeof prompt === 'string' ? prompt : prompt();
    const messages = [
        ...(opt?.messageHistory ?? []),
        { role: 'user', content: message },
    ];
    return chat(model, messages, opt);
}
exports.completion = completion;
async function chat(model, messages, _opt) {
    const jsonSchema = _opt?.schema && (0, utils_1.zodToJsonSchema)(_opt?.schema);
    const opt = (0, lodash_1.defaults)({
        callFunction: _opt?.schema
            ? _opt.functionName ?? FunctionName
            : undefined,
        functions: _opt?.schema
            ? [
                {
                    name: _opt.functionName ?? FunctionName,
                    description: _opt.functionDescription ?? FunctionDescription,
                    parameters: jsonSchema,
                },
            ]
            : undefined,
    }, _opt, Defaults);
    if (opt.schema &&
        opt.schema._def.typeName !== zod_1.z.ZodFirstPartyTypeKind.ZodObject) {
        throw new Error('Schemas can ONLY be an object');
    }
    utils_1.debug.log('⬆️ sending request:', messages);
    try {
        const hasFunctionCall = !(model instanceof llm_api_1.AnthropicChatApi ||
            model instanceof llm_api_1.AnthropicBedrockChatApi ||
            model instanceof llm_api_1.GroqChatApi);
        const schemaInstructions = !hasFunctionCall && _opt?.schema && JSON.stringify(jsonSchema);
        const firstSchemaKey = !hasFunctionCall &&
            _opt?.schema &&
            Object.keys(jsonSchema['properties'])[0];
        const responsePrefix = `\`\`\`json\n{ "${firstSchemaKey}":`;
        const stopSequence = '```';
        let response = !hasFunctionCall && _opt?.schema
            ? await model.chatCompletion(messages, {
                ...opt,
                systemMessage: `You will respond to ALL human messages in JSON. Make sure the response correctly follow the following JSON schema specifications:\n<json_schema>\n${schemaInstructions}\n</json_schema>\n\n${opt.systemMessage
                    ? typeof opt.systemMessage === 'string'
                        ? opt.systemMessage
                        : opt.systemMessage()
                    : ''}`.trim(),
                responsePrefix: opt.responsePrefix ?? responsePrefix,
                stop: stopSequence,
            })
            : await model.chatCompletion(messages, opt);
        if (!response) {
            throw new Error('Chat request failed');
        }
        !model.modelConfig.stream && utils_1.debug.log('⬇️ received response:', response);
        if (opt?.schema) {
            if (hasFunctionCall && !response.arguments) {
                if (opt.autoHeal) {
                    utils_1.debug.log('⚠️ function not called, autohealing...');
                    response = await response.respond({
                        role: 'user',
                        content: `Please respond with a call to the ${FunctionName} function`,
                    });
                    if (!response.arguments) {
                        throw new Error('Response function autoheal failed');
                    }
                }
                else {
                    throw new Error('Response function not called');
                }
            }
            let json = hasFunctionCall
                ? response.arguments
                : (0, utils_1.parseUnsafeJson)(response.content ?? '');
            if (!json) {
                throw new Error('No response received');
            }
            const res = opt.schema.safeParse(json);
            if (res.success) {
                return {
                    ...response,
                    respond: (message, opt) => chat(model, [
                        ...messages,
                        response.message,
                        typeof message === 'string'
                            ? {
                                role: hasFunctionCall ? 'tool' : 'user',
                                toolCallId: response.toolCallId,
                                content: message,
                            }
                            : message,
                    ], opt ?? _opt),
                    data: res.data,
                };
            }
            else {
                utils_1.debug.error('⚠️ error parsing response', res.error);
                if (opt.autoHeal) {
                    utils_1.debug.log('⚠️ response parsing failed, autohealing...', res.error);
                    const issuesMessage = res.error.issues.reduce((prev, issue) => issue.path && issue.path.length > 0
                        ? `${prev}\nThe issue is at path ${issue.path.join('.')}: ${issue.message}.`
                        : `\nThe issue is: ${issue.message}.`, hasFunctionCall
                        ? `There is an issue with that response, please rewrite by calling the ${FunctionName} function with the correct parameters.`
                        : `There is an issue with that response, please follow the JSON schema EXACTLY, the output must be valid parsable JSON: ${schemaInstructions}`);
                    response = await response.respond(issuesMessage);
                }
                else {
                    throw new Error('Response parsing failed');
                }
            }
            json = hasFunctionCall
                ? response.arguments
                : (0, utils_1.parseUnsafeJson)(response.content ?? '');
            if (!json) {
                throw new Error('Response schema autoheal failed');
            }
            const data = opt.schema.parse(json);
            return {
                ...response,
                respond: (message, opt) => chat(model, [
                    ...messages,
                    response.message,
                    typeof message === 'string'
                        ? {
                            role: hasFunctionCall ? 'tool' : 'user',
                            toolCallId: response.toolCallId,
                            content: message,
                        }
                        : message,
                ], opt ?? _opt),
                data,
            };
        }
        return {
            ...response,
            respond: (message, opt) => chat(model, [
                ...messages,
                response.message,
                typeof message === 'string'
                    ? { role: 'user', content: message }
                    : message,
            ], opt ?? _opt),
            data: String(response.content),
        };
    }
    catch (e) {
        if (e instanceof llm_api_1.TokenError && opt.autoSlice) {
            const message = (0, lodash_1.last)(messages)?.content ?? '';
            const chunkSize = message.length - e.overflowTokens;
            if (chunkSize < 0) {
                throw e;
            }
            utils_1.debug.log(`⚠️ Request prompt too long, splitting text with chunk size of ${chunkSize}`);
            const newMessage = message.slice(0, chunkSize);
            return chat(model, [...messages.slice(0, -1), { role: 'user', content: newMessage }], opt);
        }
        else {
            throw e;
        }
    }
}
exports.chat = chat;
